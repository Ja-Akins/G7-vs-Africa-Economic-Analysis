# ğŸŒ G7 vs. Emerging Africa: Economic Volatility & Recovery Analysis (2000â€“2024)

## Project Overview

This project delivers an **end-to-end Data Engineering and Analytics** solution comparing the economic resilience of **G7 nations** against **Africaâ€™s Top 5 Emerging Economies**. Rather than relying on static datasets, the project implements a live ETL pipeline that ingests data directly from the World Bank API, applies statistical methods to detect economic â€œshock events,â€ and leverages SQL analytics to uncover structural inefficiencies in development outcomes.

The central objective is to quantify how nations absorb, recover from, and capitalize on economic volatilityâ€”revealing the hidden trade-offs between stability and growth potential.

- **Data Source:** World Bank Open Data API (Live Fetch)  
- **Tools Used:** Python (Pandas, Concurrent Futures), SQL (MySQL), Tableau, Seaborn  

---

## ğŸ› ï¸ Data Engineering Process (Python Implementation)

This project goes beyond standard CSV analysis by implementing a full ETL pipeline capable of ingesting, transforming, and persisting 25 years of economic data across 8 indicators and 12 countries.

The pipeline is designed to be **scalable, fault-tolerant, and statistically aware**, enabling real-time data ingestion while preserving economic â€œshock eventsâ€ for analytical depth.

### ğŸ”„ Concurrent Extraction
- Implemented `concurrent.futures.ThreadPoolExecutor` to parallelize API requests, reducing ingestion time by **~70%**.
- Built pagination handling to ensure **zero data loss** across thousands of API records.
- Each countryâ€“indicator pair is fetched independently, allowing the pipeline to scale horizontally.

### ğŸ“Š Statistical Outlier Detection (IQR Method)
- For every countryâ€“indicator series, the **Interquartile Range (IQR)** is computed dynamically.
- Instead of deleting anomalies, values outside `1.5 Ã— IQR` are flagged as economic **â€œShock Eventsâ€** using an `is_outlier` column.
- This preserves data integrity while enabling volatility modeling and event-driven analysis.

### ğŸ” Secure & Optimized Loading
- Database credentials are managed via environment variables (`.env`) for security.
- Data is batch-loaded into MySQL using `SQLAlchemy`, ensuring efficient writes and transactional integrity.
- The resulting schema is analytics-ready, enabling advanced SQL window functions and longitudinal queries.

This engineering layer transforms raw API responses into a structured, query-optimized economic observatoryâ€”bridging live global data with analytical rigor.

---

## ğŸ“ File Structure

```text
.
â”œâ”€â”€ data/
â”‚   â””â”€â”€ processed/              # Cleaned CSVs generated by the pipeline
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ analysis.ipynb          # Python visualization of SQL results
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ analytical_metrics.sql  # Advanced SQL (CTEs, Window Functions)
â”œâ”€â”€ src/
â”‚   â””â”€â”€ etl_pipeline.py         # Main ETL script (API â†’ MySQL)
â”œâ”€â”€ .gitignore                  # Hides sensitive files (.env)
â”œâ”€â”€ requirements.txt            # Project dependencies
â””â”€â”€ README.md                   # Project documentation